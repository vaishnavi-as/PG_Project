# -*- coding: utf-8 -*-
"""Original_Face and Emotion Running.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NhobX3HCDsksyiSPb_vZ-A1LCIzQJuO-
"""

!pip install dlib

!pip install face_recognition

from PIL import Image
from matplotlib import pyplot as plt
import numpy as np
import face_recognition
import keras
from keras.models import load_model
import cv2

!pip freeze> requirements.txt

from google.colab import drive
drive.mount('/content/drive')
!ls "/content/drive/MyDrive/PG_Project/face_and_emotion_detection-master/test_images"

image = Image.open(r'/content/WhatsApp Image 2019-11-30 at 10.37.15 PM.jpeg')
image_array = np.array(image)
plt.imshow(image_array)

image = face_recognition.load_image_file(r'/content/1460098967_3fff6ac63f_1102_70555165@N00.jpg')
face_locations = face_recognition.face_locations(image)
image_array = np.array(image)
plt.imshow(image_array)

"""Face Detection"""

image1 = Image.open("/content/DS1.PNG")
image_array1 = np.array(image1)
plt.imshow(image_array1)

image2 = Image.open("/content/DP2.PNG")
image_array2 = np.array(image2)
plt.imshow(image_array2)

image1 = face_recognition.load_image_file(r'/content/DS1.PNG')
image2 = face_recognition.load_image_file(r'/content/DP2.PNG')

encoding_1 = face_recognition.face_encodings(image1)[0]

encoding_2 = face_recognition.face_encodings(image2)[0]

results = face_recognition.compare_faces([encoding_1], encoding_2,tolerance=0.50)

print(results)

"""Emotion Detection"""

emotion_dict= {'Angry': 0, 'Sad': 5, 'Neutral': 4, 'Disgust': 1, 'Surprise': 6, 'Fear': 2, 'Happy': 3}

model = load_model(r'/content/model_v6_23.hdf5')

import numpy
from matplotlib import pyplot as plt

face_count = 1
plt.ion()
for face_count in range(0, len(face_locations)):
  
  top, right, bottom, left = face_locations[face_count]
  #print(face_locations[face_count])
  face_image1 = image[top:bottom, left:right]

  #print(face_image1.shape)
  y = numpy.dot(face_image1.shape, face_count)
  plt.figure()
  plt.imshow(face_image1)

  # resizing the image
  face_image12 = cv2.resize(face_image1, (48,48))
  face_image12 = cv2.cvtColor(face_image12, cv2.COLOR_BGR2GRAY)
  face_image12 = np.reshape(face_image12, [1, face_image12.shape[0], face_image12.shape[1], 1])
  predicted_class = np.argmax(model.predict(face_image12))
  label_map = dict((v,k) for k,v in emotion_dict.items()) 
  predicted_label = label_map[predicted_class]
  #print(predicted_label)

  #y = numpy.dot(face_image1.shape, face_count)
  #plt.figure()
  #plt.imshow(face_image1)
  print(face_count)
  print(predicted_label)

